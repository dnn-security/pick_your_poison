## Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification

<p>
    <a href="https://www.python.org/downloads/">
            <img alt="Build" src="https://img.shields.io/badge/3.10-Python-blue">
    </a>
    <a href="https://pytorch.org">
            <img alt="Build" src="https://img.shields.io/badge/1.11-PyTorch-orange">
    </a>

</p>

This repository will contain the code including pre-trained models for our paper "Pick your Poison: Undetectability versus Robustness in Data
Poisoning Attacks against Deep Image Classification". 
We implement all backdoor detection & repair methods and attacks presented in the paper.

**(Jun 4) The code will be released soon.**


## Preprint
> **Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification** 
> Nils Lukas and Florian Kerschbaum.
> Preprint.
> 
> [![arXiv](https://img.shields.io/badge/arXiv-2302.00539-green)]([https://arxiv.org/abs/2304.07361](https://arxiv.org/abs/2305.09671))


## Bibtex
Please consider citing the following papers if you found our work useful.  
```
@article{lukas2023pick,
  title={Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification},
  author={Lukas, Nils and Kerschbaum, Florian},
  journal={arXiv preprint arXiv:2305.09671},
  year={2023}
}
```
